{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "29cb46d7",
      "metadata": {
        "id": "29cb46d7"
      },
      "source": [
        "### Run this cell to install commands"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab_tpot_tutorial.ipynb)"
      ],
      "metadata": {
        "id": "zJTaC7o-laaA"
      },
      "id": "zJTaC7o-laaA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaee9fda",
      "metadata": {
        "id": "eaee9fda",
        "outputId": "157d072a-958f-4e4c-ce78-f396e4505bbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.5)\n",
            "Collecting tpot\n",
            "  Downloading TPOT-0.11.7-py3-none-any.whl (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.1.0)\n",
            "Collecting stopit>=1.1.1\n",
            "  Downloading stopit-1.1.2.tar.gz (18 kB)\n",
            "Collecting update-checker>=0.16\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.3.5)\n",
            "Collecting xgboost>=1.1.0\n",
            "  Downloading xgboost-1.5.2-py3-none-manylinux2014_x86_64.whl (173.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 173.6 MB 7.5 kB/s \n",
            "\u001b[?25hCollecting deap>=1.2\n",
            "  Downloading deap-1.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 58.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.21.5)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.7/dist-packages (from tpot) (4.63.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->tpot) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->tpot) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.2->tpot) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.0->tpot) (3.1.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from update-checker>=0.16->tpot) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.24.3)\n",
            "Building wheels for collected packages: stopit\n",
            "  Building wheel for stopit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stopit: filename=stopit-1.1.2-py3-none-any.whl size=11956 sha256=1ff5840bcfcdab2034426e3ecca92e14be7efba128bbdceba07fbb429e579bfe\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/d2/79/eaf81edb391e27c87f51b8ef901ecc85a5363dc96b8b8d71e3\n",
            "Successfully built stopit\n",
            "Installing collected packages: xgboost, update-checker, stopit, deap, tpot\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "Successfully installed deap-1.3.1 stopit-1.1.2 tpot-0.11.7 update-checker-0.18.0 xgboost-1.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install xgboost\n",
        "!pip install tpot\n",
        "# Hi this is jamie can you see this?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60aa22cd",
      "metadata": {
        "id": "60aa22cd"
      },
      "source": [
        "* What is TPOT?\n",
        "    * TPOT is an automated machine learning tool that utilizes genetic programming to optimize machine learning pipelines. It is essentially an assistant for tree based pipeline optimization. <br/>\n",
        "<br/>\n",
        "\n",
        "* What/Who is it good for? \n",
        "    * TPOT rids you of having to do the most tedious portion of machine learning. It does this by exploring many varieties of pipelines and finding the best one for the data you are working with. \n",
        "    * This AutoML tool is an unbeatable asset and is a real bargain if you want to get a classification accuracy which is very competitive. Over and above that, this tool can identify artificial feature constructors that can enhance the classification accuracy in a very demanding way by identifying novel pipeline operators. The operators of TPOT are chained together to develop a series of operations acting on the given dataset <br/>\n",
        "    * TPOT can be used for both classification and regression.\n",
        "<br/>\n",
        "<br/>\n",
        "* How to Install\n",
        "    * We used the call `pip install tpot` in order to install TPOT. It is noted to have PyTorch installed as well, but it is not necessary. The installation for PyTorch is `pip install torch` <br/>\n",
        "<br/>\n",
        "\n",
        "Link to another TPOT tutorial: https://machinelearningmastery.com/tpot-for-automated-machine-learning-in-python/\n",
        "\n",
        "Links to additional documentation: <br/>\n",
        "http://epistasislab.github.io/tpot/ <br/>\n",
        "https://github.com/EpistasisLab/tpot\n",
        "\n",
        "\n",
        "#### Remain target to class – important step"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec644eb3",
      "metadata": {
        "id": "ec644eb3"
      },
      "source": [
        "### Run this cell to import all of the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "022ab1ce",
      "metadata": {
        "id": "022ab1ce"
      },
      "outputs": [],
      "source": [
        "from tpot import TPOTClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd \n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1b0e863",
      "metadata": {
        "id": "f1b0e863"
      },
      "source": [
        "## Example 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2898aaa9",
      "metadata": {
        "id": "2898aaa9",
        "outputId": "bc4a222c-6cab-41ce-d8aa-d56bcaf69112"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[5.1, 3.5, 1.4, 0.2],\n",
              "        [4.9, 3. , 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.3, 0.2],\n",
              "        [4.6, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.6, 1.4, 0.2]]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#load in all of the data\n",
        "iris = load_iris()\n",
        "iris.data[0:5], iris.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d1ac27a",
      "metadata": {
        "id": "1d1ac27a",
        "outputId": "b8c11070-d1d9-476b-c2a4-d84cf5f77482"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((112, 4), (38, 4), (112,), (38,))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#split data into a test and train data set\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, train_size=0.75, test_size=0.25)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "015515d0",
      "metadata": {
        "id": "015515d0",
        "outputId": "434e990f-11e1-442d-fcdb-96169c5bbb6a",
        "colab": {
          "referenced_widgets": [
            "49da94ec3a20499782a4a98ab6224166"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49da94ec3a20499782a4a98ab6224166",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Optimization Progress:   0%|          | 0/100 [00:00<?, ?pipeline/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "2.21 minutes have elapsed. TPOT will close down.\n",
            "TPOT closed during evaluation in one generation.\n",
            "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
            "\n",
            "\n",
            "TPOT closed prematurely. Will use the current best pipeline.\n",
            "\n",
            "Best pipeline: MLPClassifier(input_matrix, alpha=0.0001, learning_rate_init=0.01)\n",
            "0.9736842105263158\n"
          ]
        }
      ],
      "source": [
        "# Fit the model based on the training data, get a score based on testing data.\n",
        "# Will report the score of the best found pipeline\n",
        "# Change max_time_mins to a higher time to allow TPOT to run without interuption\n",
        "# It is currently at 2 mins for sake of not taking to long\n",
        "tpot = TPOTClassifier(verbosity=2, max_time_mins=2)\n",
        "tpot.fit(X_train, y_train)\n",
        "print(tpot.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fedcae2c",
      "metadata": {
        "id": "fedcae2c"
      },
      "outputs": [],
      "source": [
        "#export the pipeline created for future use\n",
        "tpot.export('tpot_iris_pipeline.py')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efa3269b",
      "metadata": {
        "id": "efa3269b"
      },
      "source": [
        "## Example 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2ac0eda",
      "metadata": {
        "id": "f2ac0eda",
        "outputId": "ff0f8184-3b1c-4ff8-dc79-93d6601aab44"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  \\\n",
              "0            1         0       3   \n",
              "1            2         1       1   \n",
              "2            3         1       3   \n",
              "3            4         1       1   \n",
              "4            5         0       3   \n",
              "\n",
              "                                                Name     Sex   Age  SibSp  \\\n",
              "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
              "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
              "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
              "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
              "4                           Allen, Mr. William Henry    male  35.0      0   \n",
              "\n",
              "   Parch            Ticket     Fare Cabin Embarked  \n",
              "0      0         A/5 21171   7.2500   NaN        S  \n",
              "1      0          PC 17599  71.2833   C85        C  \n",
              "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
              "3      0            113803  53.1000  C123        S  \n",
              "4      0            373450   8.0500   NaN        S  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#read in data\n",
        "titanic = pd.read_csv('titanic_train.csv')\n",
        "titanic.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30eeb3aa",
      "metadata": {
        "id": "30eeb3aa"
      },
      "outputs": [],
      "source": [
        "#rename target variable to \"class\", in this case that is the 'survivor' column\n",
        "titanic.rename(columns={'Survived': 'class'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcc561a3",
      "metadata": {
        "id": "bcc561a3",
        "outputId": "f0dbea09-4e5f-4aa0-f2cd-ee6692642ff2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of levels in category 'Name': \b 891.00 \n",
            "Number of levels in category 'Sex': \b 2.00 \n",
            "Number of levels in category 'Ticket': \b 681.00 \n",
            "Number of levels in category 'Cabin': \b 148.00 \n",
            "Number of levels in category 'Embarked': \b 4.00 \n"
          ]
        }
      ],
      "source": [
        "# Find out how many different categories there are for each of these 5 features\n",
        "for cat in ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']:\n",
        "    print(\"Number of levels in category '{0}': \\b {1:2.2f} \".format(cat, titanic[cat].unique().size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5be7251f",
      "metadata": {
        "id": "5be7251f",
        "outputId": "c40c2078-dfdd-405c-800b-32b17087fd0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Levels for catgeory 'Sex': ['male' 'female']\n",
            "Levels for catgeory 'Embarked': ['S' 'C' 'Q' nan]\n"
          ]
        }
      ],
      "source": [
        "#print out what those categories are for 'Sex' and 'Embarked'\n",
        "for cat in ['Sex', 'Embarked']:\n",
        "    print(\"Levels for catgeory '{0}': {1}\".format(cat, titanic[cat].unique()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bb50fa8",
      "metadata": {
        "id": "8bb50fa8"
      },
      "outputs": [],
      "source": [
        "# Map the categories to numerical values\n",
        "titanic['Sex'] = titanic['Sex'].map({'male':0,'female':1})\n",
        "titanic['Embarked'] = titanic['Embarked'].map({'S':0,'C':1,'Q':2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c06d01",
      "metadata": {
        "id": "11c06d01",
        "outputId": "a7b2ddcc-778b-4d11-ee53-c3294251c3ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PassengerId    False\n",
              "class          False\n",
              "Pclass         False\n",
              "Name           False\n",
              "Sex            False\n",
              "Age            False\n",
              "SibSp          False\n",
              "Parch          False\n",
              "Ticket         False\n",
              "Fare           False\n",
              "Cabin          False\n",
              "Embarked       False\n",
              "dtype: bool"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fill na values and then double check there are non left\n",
        "titanic = titanic.fillna(-999)\n",
        "pd.isnull(titanic).any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa017a2a",
      "metadata": {
        "id": "aa017a2a"
      },
      "outputs": [],
      "source": [
        "# Encode categorical features, specifically 'Cabin'\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "mlb = MultiLabelBinarizer()\n",
        "CabinTrans = mlb.fit_transform([{str(val)} for val in titanic['Cabin'].values])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34567d07",
      "metadata": {
        "id": "34567d07",
        "outputId": "3155944d-e16e-49f9-ff5a-fb9d581f93b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CabinTrans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f31db644",
      "metadata": {
        "id": "f31db644"
      },
      "outputs": [],
      "source": [
        "# drop features that we won't use\n",
        "titanic_new = titanic.drop(['Name','Ticket','Cabin','class'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8ccd33c",
      "metadata": {
        "id": "e8ccd33c"
      },
      "outputs": [],
      "source": [
        "#check correct encoding done\n",
        "assert (len(titanic['Cabin'].unique()) == len(mlb.classes_)), \"Not Equal\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "594776f6",
      "metadata": {
        "id": "594776f6"
      },
      "outputs": [],
      "source": [
        "#add CabinTrans to the features we kept\n",
        "# stack the arrays column wise\n",
        "titanic_new = np.hstack((titanic_new.values,CabinTrans))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8d47aef",
      "metadata": {
        "id": "e8d47aef",
        "outputId": "d7e13302-1252-47c1-e10c-8732e706971a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# make sure there are no nas in the data\n",
        "np.isnan(titanic_new).any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fb6b7df",
      "metadata": {
        "id": "6fb6b7df",
        "outputId": "aca92fc8-825c-48aa-f68e-9c381cf26095"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "156"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "titanic_new[0].size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09fe6803",
      "metadata": {
        "id": "09fe6803"
      },
      "outputs": [],
      "source": [
        "# get the class ('survivor') values\n",
        "titanic_class = titanic['class'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a14bb5fd",
      "metadata": {
        "id": "a14bb5fd",
        "outputId": "45091784-bab3-4d84-c16f-eb833ce2a54f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(668, 223)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# split the data into testing and training- this will give us indicies\n",
        "training_indices, validation_indices = training_indices, testing_indices = train_test_split(titanic.index, stratify = titanic_class, train_size=0.75, test_size=0.25)\n",
        "training_indices.size, validation_indices.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "227863a0",
      "metadata": {
        "id": "227863a0",
        "outputId": "29412f3b-bc04-475b-b8f6-d68c1c89875d",
        "colab": {
          "referenced_widgets": [
            ""
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Optimization Progress:   0%|          | 0/40 [00:00<?, ?pipeline/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generation 1 - Current best internal CV score: 0.8068791381438671\n",
            "\n",
            "2.01 minutes have elapsed. TPOT will close down.\n",
            "TPOT closed during evaluation in one generation.\n",
            "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
            "\n",
            "\n",
            "TPOT closed prematurely. Will use the current best pipeline.\n",
            "\n",
            "Best pipeline: DecisionTreeClassifier(input_matrix, criterion=gini, max_depth=3, min_samples_leaf=13, min_samples_split=20)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TPOTClassifier(max_eval_time_mins=0.04, max_time_mins=2, population_size=40,\n",
              "               verbosity=2)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create the classifier and fit the model, reports the best pipeline\n",
        "# Parameters within the TPOT Classifier can be changed to allow for longer run time across more models\n",
        "tpot = TPOTClassifier(verbosity=2, max_time_mins=2, max_eval_time_mins=0.04, population_size=40)\n",
        "tpot.fit(titanic_new[training_indices], titanic_class[training_indices])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca18f35b",
      "metadata": {
        "id": "ca18f35b",
        "outputId": "154beb98-2008-49a7-9200-9f6f60ce8e13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.757847533632287"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#gives the score from the best pipeline\n",
        "tpot.score(titanic_new[validation_indices], titanic.loc[validation_indices, 'class'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d710e07",
      "metadata": {
        "id": "4d710e07"
      },
      "outputs": [],
      "source": [
        "#export the best pipeline for future use\n",
        "tpot.export('tpot_titanic_pipeline.py')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7ff45ed",
      "metadata": {
        "id": "d7ff45ed",
        "outputId": "18190ac3-fadb-42f9-ca12-9735eda9199a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>418.000000</td>\n",
              "      <td>418.000000</td>\n",
              "      <td>332.000000</td>\n",
              "      <td>418.000000</td>\n",
              "      <td>418.000000</td>\n",
              "      <td>417.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1100.500000</td>\n",
              "      <td>2.265550</td>\n",
              "      <td>30.272590</td>\n",
              "      <td>0.447368</td>\n",
              "      <td>0.392344</td>\n",
              "      <td>35.627188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>120.810458</td>\n",
              "      <td>0.841838</td>\n",
              "      <td>14.181209</td>\n",
              "      <td>0.896760</td>\n",
              "      <td>0.981429</td>\n",
              "      <td>55.907576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>892.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>996.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.895800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1100.500000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.454200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1204.750000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>31.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1309.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>76.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>512.329200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       PassengerId      Pclass         Age       SibSp       Parch        Fare\n",
              "count   418.000000  418.000000  332.000000  418.000000  418.000000  417.000000\n",
              "mean   1100.500000    2.265550   30.272590    0.447368    0.392344   35.627188\n",
              "std     120.810458    0.841838   14.181209    0.896760    0.981429   55.907576\n",
              "min     892.000000    1.000000    0.170000    0.000000    0.000000    0.000000\n",
              "25%     996.250000    1.000000   21.000000    0.000000    0.000000    7.895800\n",
              "50%    1100.500000    3.000000   27.000000    0.000000    0.000000   14.454200\n",
              "75%    1204.750000    3.000000   39.000000    1.000000    0.000000   31.500000\n",
              "max    1309.000000    3.000000   76.000000    8.000000    9.000000  512.329200"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Read in the test set that hasn't been touched yet\n",
        "titanic_sub = pd.read_csv('titanic_test.csv')\n",
        "titanic_sub.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8264d2ed",
      "metadata": {
        "id": "8264d2ed"
      },
      "outputs": [],
      "source": [
        "# clean the data and remove any nas\n",
        "for var in ['Cabin']: #,'Name','Ticket']:\n",
        "    new = list(set(titanic_sub[var]) - set(titanic[var]))\n",
        "    titanic_sub.loc[titanic_sub[var].isin(new), var] = -999"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe8198e3",
      "metadata": {
        "id": "fe8198e3"
      },
      "outputs": [],
      "source": [
        "# encode sex and embarked to numerical values\n",
        "titanic_sub['Sex'] = titanic_sub['Sex'].map({'male':0,'female':1})\n",
        "titanic_sub['Embarked'] = titanic_sub['Embarked'].map({'S':0,'C':1,'Q':2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13204313",
      "metadata": {
        "id": "13204313",
        "outputId": "be01f8fb-0c0c-4dce-d0f1-47c366f74b42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PassengerId    False\n",
              "Pclass         False\n",
              "Name           False\n",
              "Sex            False\n",
              "Age            False\n",
              "SibSp          False\n",
              "Parch          False\n",
              "Ticket         False\n",
              "Fare           False\n",
              "Cabin          False\n",
              "Embarked       False\n",
              "dtype: bool"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fill the nas and double check none are left\n",
        "titanic_sub = titanic_sub.fillna(-999)\n",
        "pd.isnull(titanic_sub).any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82e8d3fb",
      "metadata": {
        "id": "82e8d3fb"
      },
      "outputs": [],
      "source": [
        "# Encode categorical features, specifically 'Cabin', drop select columns to create a new dataframe\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "mlb = MultiLabelBinarizer()\n",
        "SubCabinTrans = mlb.fit([{str(val)} for val in titanic['Cabin'].values]).transform([{str(val)} for val in \n",
        "                                                                                    titanic_sub['Cabin'].values])\n",
        "titanic_sub = titanic_sub.drop(['Name','Ticket','Cabin'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "185ba8c1",
      "metadata": {
        "id": "185ba8c1"
      },
      "outputs": [],
      "source": [
        "# combine slimmed dataframe with cabin now encoded\n",
        "titanic_sub_new = np.hstack((titanic_sub.values,SubCabinTrans))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "359c8b6b",
      "metadata": {
        "id": "359c8b6b",
        "outputId": "9aa495a3-1454-4bc9-9d15-ac7dfcaf7d8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.any(np.isnan(titanic_sub_new))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e73a0c32",
      "metadata": {
        "id": "e73a0c32"
      },
      "outputs": [],
      "source": [
        "assert (titanic_new.shape[1] == titanic_sub_new.shape[1]), \"Not Equal\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d868e452",
      "metadata": {
        "id": "d868e452"
      },
      "outputs": [],
      "source": [
        "# predict the class based on the data given\n",
        "submission = tpot.predict(titanic_sub_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1666f357",
      "metadata": {
        "id": "1666f357",
        "outputId": "a0bc6567-a574-45bb-fbe8-0be1c674521a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 0, 0, 1, 0, 1, 0, 1, 0])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "submission[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d91d737",
      "metadata": {
        "id": "3d91d737"
      },
      "outputs": [],
      "source": [
        "#create a data frame with passenger id and what class they belong to (if they survived or not)\n",
        "#save as csv\n",
        "final = pd.DataFrame({'PassengerId': titanic_sub['PassengerId'], 'Survived': submission})\n",
        "final.to_csv('data/submission.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "240feb73",
      "metadata": {
        "id": "240feb73",
        "outputId": "894c1558-ba3f-44f3-e166-fde2b41e285f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(418, 2)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12aac202",
      "metadata": {
        "id": "12aac202"
      },
      "source": [
        "### References\n",
        "\n",
        "https://github.com/EpistasisLab/tpot"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "Copy of tpot_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "12aac202"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}